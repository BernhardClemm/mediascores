---
author: "Gregory Eady"
date: "Last updated: `r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: false
    toc_float: false
    theme: default
    highlight: default
bibliography: "Bibliography.bib"
biblio-style: apsr
comment: ""
vignette: >
  %\VignetteIndexEntry{misreport Vignette}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
params:
  EVAL: !r identical(Sys.getenv("NOT_CRAN"), "true")
---
 
<!--
To build:
Anywhere there is a comment "SET TO EVAL = TRUE TO BUILD VIGNETTE", set that R code block to "eval = TRUE", then run:

rmarkdown::render("~/GitHub/mediascores/vignettes/mediascores-vignette.Rmd")

To submit changes to GitHub or CRAN, however, RESET eval in those cases to "eval = FALSE"
-->


<link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700,900' rel='stylesheet' type='text/css'>

<script type="text/x-mathjax-config"> 
    MathJax.Hub.Config({ 
        "HTML-CSS": { scale: 91, linebreaks: { automatic: true } }, 
        SVG: { linebreaks: { automatic:true } }, 
        displayAlign: "center" });
</script>

<style>
  @media {
    .container-fluid{
      max-width: 800px;
    }
  }

  body {
    background: #F6F6F6;
    font-family: 'Open Sans', sans-serif;
    font-size: 17px;
  }

  mark {
    background-color: #E9E9E9;
    color: #000000;
  }

  pre {
    background-color: #FFFFFF;
    border-radius: 0px;
    border-width: 1px;
    border-style: dotted;
    border-color: #3B3B3B;

  }

  p {
    padding-top: 10px;
  }

  .container-fluid h1,
  .container-fluid h2,
  .container-fluid h3,
  .container-fluid h4{
    color:#000000;
    font-weight: 600;
    font-family: 'Open Sans', sans-serif;
  }

  .container-fluid h1{
    text-align: left;
    letter-spacing: 1px;
    line-height: 130%;
    font-weight: 900;
    font-size: 32px;
    padding-top:10px;
    padding-bottom:5px;
  }

  .container-fluid h2{
    text-align: left;
    letter-spacing: 1px;
    font-weight: 900;
    font-size: 18px;
    padding-bottom:5px;
  }

  blockquote {
    border-left: 0px solid;
    padding-left: 40px;
    padding-top: 14px;
    padding-bottom: 18px;
    padding-right: 40px;
    background-color: #F6F6F6;
    border-top: 0px;
    border-bottom: 0px;
    margin: 0px;
    background-position: middle left;
    background-repeat: no-repeat;
    text-indent: 0px;
    font-size: 16px;
    letter-spacing: 0px;
    /*line-height: 22px;*/
    font-family: 'Lato', sans-serif;
  }

  a:link,
  a:visited,
  a:hover,
  a:active {
    color: #db0057;
  }

  hr {
    width: 100%;
    margin-left: 0; 
    margin-right: auto; 
    height: 0px;
    border: 1px;
    color: #db0057;
    border-top: dotted 1px;
  }

</style>


```{r set-options, include = FALSE}
options(width = 10000)
knitr::opts_chunk$set(comment = "")
set.seed(121)
my.theme <- function(base_size = 10, base_family = "",
                     remove.ticks.x = FALSE,
                     remove.ticks.y = FALSE,
                     grid.x_colour = NA, grid.y_colour = NA,
                     grid.x_linetype = NA, grid.y_linetype = NA,
                     grid.x_size = NA, grid.y_size = NA,
                     strip_colour = NA, strip_text = "black",
                     background_colour = NA,
                     plot_background_colour = NA,
                     text_colour = NA,
                     tick_colour = "black", tick_length = 0.2,
                     borderless = 0, bordersize = 0.5){ 
    if((!is.na(grid.x_linetype) | !is.na(grid.x_size)) & is.na(grid.x_colour)) {
        grid.x_colour <- "black"
    }
    if((!is.na(grid.y_linetype) | !is.na(grid.y_size)) & is.na(grid.y_colour)) {
        grid.y_colour <- "black"
    }

  if(is.na(grid.x_size)) grid.x_size <- 0.25
  if(is.na(grid.x_linetype)) grid.x_linetype <- 1
  if(is.na(grid.y_size)) grid.y_size <- 0.25
  if(is.na(grid.y_linetype)) grid.y_linetype <- 1

  if(borderless == 2) {
    border <- theme(panel.border = element_blank(),
                    strip.background = element_blank())
  }
  
  else if(borderless == 1) {
    border <- theme(axis.line = element_line(colour = "black", size = 0.25),
                    panel.border = element_blank(),
                    # panel.background = element_blank(),
                    strip.background = element_blank())
  }
  else if(borderless == 0) border <- theme()
  if(remove.ticks.x == TRUE) border <- border + theme(axis.ticks.x = element_blank())
  if(remove.ticks.y == TRUE) border <- border + theme(axis.ticks.y = element_blank())
  theme(axis.text.x       = element_text(family = base_family, colour = "black", size = base_size, vjust = 1, lineheight = 0.9),
        axis.text.y       = element_text(family = base_family, colour = "black", size = base_size, hjust = 1, lineheight = 0.9),
        axis.ticks        = element_line(colour = tick_colour, size = 0.2),
        axis.ticks.length = unit(tick_length, "lines"),
        axis.title.x      = element_text(family = base_family, face = "bold", size = base_size*0.9, colour = "black", vjust = 0),
        axis.title.y      = element_text(family = base_family, face = "bold", size = base_size*0.9, angle = 90, colour = "black", vjust = 1),
        legend.background = element_blank(),#element_rect(fill = "grey95"),
        legend.key = element_blank(),
        legend.key.size = unit(0.6, "lines"),
        legend.text = element_text(family = base_family, size = base_size, color = "black", face = "plain", lineheight = 1),
        legend.text.align = -1, 
        legend.title = element_blank(),
        legend.title.align = 1,
        legend.position = "top",
        legend.direction = "horizontal",
        legend.justification = c(0, 1),
        panel.background = element_rect(fill = NA),
        panel.border = element_rect(fill = "transparent", colour = "black", size = bordersize),
        panel.grid.major.x = element_line(colour = grid.x_colour, size = grid.x_size, linetype = grid.x_linetype),
        panel.grid.major.y = element_line(colour = grid.y_colour, size = grid.y_size, linetype = grid.y_linetype),
        panel.grid.minor = element_blank(),
        panel.spacing = unit(0.75, "lines"),
        strip.background = element_blank(),
        strip.text.x = element_text(family = base_family, size = base_size, face = "bold", colour = strip_text),
        strip.text.y = element_text(family = base_family, size = base_size, face = "plain", angle = -90, colour = strip_text),
        plot.background = element_rect(fill = NA, colour = NA),
        plot.title = element_text(family = base_family, size = base_size * 1.1, vjust = 0.5, hjust = 0, face = "bold")) +
    border
}
```


<!-- BEGIN DOCUMENT -->

<h1 style = "font-size: 28px; margin-bottom: 0px; padding-bottom:0px;">News-sharing Ideology from Social Media Link Data</h1>
<h2 style = "font-size: 24px; margin-top: 15px">The [`mediascores`](https://cran.r-project.org/package=mediascores){target="_blank"} library in R</h2>
<h5 style = "font-size: 18px; margin-top: 30px; font-weight: 300">**[Gregory Eady](https://google.com){target="_blank"}**^$\dagger$^ and **[Fridolin Linder](https://google.com){target="_blank"}**^$\dagger$^

^$\dagger$^ NYU [Social Media and Political Participation (SMaPP) Lab](https://smappnyu.org/){target="_blank"} & Department of Politics</h5>
<hr style = "margin-bottom: 0px; margin-top: 18px; border:1px; border-top: dotted 1px; color:#006400">


# 1. Introduction {#section1}

This document provides a brief introduction to the R library [`mediascores`](https://cran.r-project.org/package=mediascores){target="_blank"}, which implements a statistical model for news media sharing data on social media (e.g. Twitter, Facebook, Reddit). The goal of the method is to provide a measure of (1) the news sharing ideology of politicians (e.g. Members of Congress) and ordinary users on social media, and (2) the ideology of the content that they share (e.g. nytimes.com, cnn.com, foxnews.com).

The logic behind the method is intuitive, and is based on a straightforward assumption of homophily: that politicians and ordinary users on social media are more likely to share news stories that are ideology close to themselves, and avoid sharing stories that are ideologically distant. In the US context, for example, a user who is liberal can be assumed to share more content from the New York Times or MSNBC than they are content from the Wall Street Journal or Fox News. Conservatives, by contrast, will be more likely to share more content from the Wall Street Journal and Fox News than from the New York Times or MSNBC.

Below, we work through an application of the method to Twitter data collected from politicians during the 115^th^ Congress. Our example shows how researchers can use these or analogous data to calculate the news-sharing ideology of social media users---in this case, politicians---and the ideology of the content they themselves share.

# 2. Data {#section2}

As data, the model uses the number of news media stories that are shared by social media users. For the purpose of this vignette, we use the stories shared by members of the 115^th^ US Congress, US governors, and members of the US executive (e.g. the current and former Presidents). The data come from the time lines of these politicians, which were collected using the Twitter API near the end of 2018. The URLs from each time line were first extracted and unshortened as necessary (e.g. [`nyti.ms/2BUTshD`](https://nyti.ms/2BUTshD){target="_blank"} ---> [`nytimes.com/2018/...`](https://www.nytimes.com/2018/02/13/upshot/fake-news-and-bots-may-be-worrisome-but-their-political-power-is-overblown.html){target="_blank"}).^[An example of a (parallelized) link unshortener is that developed by [Leon Yin](https://leonyin.org/){target="_blank"}: [`urlExpander`](https://github.com/SMAPPNYU/urlExpander){target="_blank"}.] The resulting matrix, as described in further detail below, contains all URLs from tweets and retweets (quote-tweets excluded) that linked to national news media.

First, we read in the `data.frame`, which contains the number of stories that have been tweeted by each politician, in addition to basic identifying information about each actor:

```{r}
library(mediascores)
library(ggplot2)

# Read in Twitter URL-sharing data from members of the 115th Congress (among other politicians)
data(MOC115)
```

The identifying information are contained in the first 6 variables of the data set:

* `name` e.g. "Nancy Pelosi"
* `nominate_name` e.g. "PELOSI, Nancy"
* `party` {Democrat, Republican}
* `group` {1 = Democrat, 2 = Republican}
* `role` {Senator, House, Governor, Delegate, Other}
* `nominate` NOMINATE score of Members of Congress^[NOMINATE scores are the canonical measure of Members of Congress's ideology, based on their roll-call voting record [@Poole1985; @Poole2005]. NOMINATE data were downloaded from [VoteView](https://voteview.com){target="_blank"}.]

The remaining columns represent the data used to estimate the ideology of users and news media organizations, and represent the count of the news stories tweeted or retweeted by each user (i.e. politician). To make this concrete, a subset of these columns is shown alongside the names of five well-known US politicians:

```{r}
columns_to_show <- c("name", "thenation.com", "huffingtonpost.com",
                     "washingtonpost.com", "foxnews.com", "breitbart.com")
politicians_to_show <- c("Ted Cruz", "Paul Ryan", "Susan Collins",
                         "Nancy Pelosi", "Bernie Sanders")
print(MOC115[MOC115$name %in% politicians_to_show, columns_to_show], row.names = FALSE)
```

For observers of American politics, these data are as one might expect: Bernie Sanders---a relatively far left politician---shares far more news from the left (e.g. thenation.com, huffingtonpost.com) than he does that from the right (e.g. foxnews.com, breitbart.com). Ted Cruz---a relatively far right politician---by contrast, shares mostly news from the ideological right.

The data used for the model, therefore, is a straightforward user-domain count matrix, where the rows denote users, the columns news media domains, and each cell represents the number of each domain that has been shared by each user. These form the core set of data for the model, and can be augmented if desired by a group indicator variable to indicate whether a given user is, for example, a Democrat, Republican, or ordinary user.


# 3. A Statistical Model of News Sharing {#section3}

To model these data, the [`mediascores`](https://cran.r-project.org/package=mediascores){target="_blank"} library is designed to fit the following model:\begin{equation}
y_{img} \sim \text{NegBin}(\pi_{img}, \omega_i\omega_m)
\end{equation}\begin{equation}
\pi_{img} = \text{exp}(\alpha_i + \gamma_m - ||\vartheta_i - \zeta_m||^2),
\end{equation}

where $y_{img}$ denotes the count of news stories from domain $m$ shared by user $i$ who belongs to group $g \in \{$Republican, Democrat$\}$ (i.e. the data in the data.frame shown above), $\alpha_i$ denotes a user-specific intercept, $\gamma_m$ denotes a domain-specific intercept, and $\omega_i$ and $\omega_m$ denote user- and domain-specific dispersion parameters respectively. The quantities of interest are denoted by $\vartheta_i$, which represents the news sharing ideology of user $i$, and $\zeta_m$, which represents the ideology of news media organization $m$. As the term $-||\vartheta_i - \zeta_m||^2$ shows, the larger the ideological spatial distance between a user $i$ and news media organization $m$ the less likely the user is to share stories from that organization.

Because the model is fit in a Bayesian context, the priors placed on the parameters in the model are as follows: \begin{equation}
\alpha_i \sim \text{Normal}(\mu_{\alpha}, \sigma_{\alpha})
\end{equation}\begin{equation}
\gamma_i \sim \text{Normal}(0, \sigma_{\gamma})
\end{equation}

Uniform prior distributions are placed on the hyperparameters $\mu_{\alpha}$, $\sigma_{\alpha}$ and $\sigma_{\gamma}$.

Group-level information, $g$ (e.g. Democrat, Republican) is then incorporated by placing separate common prior distributions on the parameters denoting the ideology, $\vartheta_{ig}$, of politicians who are members of a given group: \begin{equation}
\vartheta_{ig} \sim \text{Normal}(\mu_{\vartheta}^{(g)}, \sigma_{\vartheta}^{(g)})
\end{equation}

Uniform prior distributions are placed on the hyperparameters $\mu_{\vartheta}^{(g)}$ and $\sigma_{\vartheta}^{(g)}$. Finally, the variance parameters $\omega_i$ and $\omega_m$, are given a common distribution $\omega_i \sim \text{InvGamma}(\omega^{(i)}_a, \omega^{(i)}_b)$ and $\omega_m \sim \text{InvGamma}(\omega^{(m)}_a, \omega^{(m)}_b)$, where uniform priors are placed on the hyperparameters.

For identification, the parameters representing news media ideology are given a standard normal distribution, $\zeta_m \sim \text{Normal}(0, 1)$. Lastly, the direction of the model needs to be set, such that high values represent ideological liberalism or conservatism. Here, we follow the practical solution of @Jackman2001, who suggests allowing the sampler to freely explore the posterior and settle in on one of the two scale directions. We then flip the scale and associated parameters after estimation (if required) depending on anchors defined by the researcher: the user is asked to set two news media domains for this purpose (e.g. \{nytimes.com, foxnews.com\}), such that the ideology of the first media organization defines the low end of the scale, and the second, the high end. In practice, this strategy works exceptionally well.


# 4. Model Fitting {#section4}

To fit the model, we use the user-domain count matrix as shown in Section 3. To obtain this matrix, we transform the `data.frame` into a matrix, first removing the columns that provide information about each politician (e.g. name, party, nominate score), which are the first 6 columns of the data set:

```{r}
# Create user-domain count matrix
User_Domain_Counts <- as.matrix(MOC115[, -c(1:6)])

# For reference, name rows by politician's name
rownames(User_Domain_Counts) <- MOC115$name

# Show sub-matrix of first 5 politicians and 5 domains:
print(User_Domain_Counts[1:5, 1:5], row.names = FALSE)
```

We can now fit the model by inputting the user-domain count matrix in the workhorse function of the [`mediascores`](https://cran.r-project.org/package=mediascores){target="_blank"} library, `mediascores()`, as follows:

```{r eval = FALSE}
# Fit the sharing ideology model
# anchor = c(1, 2) because column 1 in the matrix represents the NY Times
# and column 2 Fox News. As a result lower values of theta_i and zeta_m will
# represent the liberal side of the scale; higher values, the conservative side.
fitted_model <- mediascores(Y = User_Domain_Counts, group = MOC115$group,
                            variational = FALSE, user_variance = FALSE,
                            chains = 4, cores = 4, threads = 4,
                            warmup = 750, iter = 1500, seed = 1,
                            open_progress = TRUE,
                            anchors = c(1, 2))
```

```{r echo = FALSE, eval = FALSE}
# Fit the sharing ideology model
# anchor = c(1, 2) because column 1 in the matrix represents the NY Times
# and column 2 Fox News. As a result lower values of theta_i and zeta_m will
# represent the liberal side of the scale; higher values, the conservative side.
system.time({fitted_model <- mediascores(Y = User_Domain_Counts, group = MOC115$group,
                            variational = FALSE, user_variance = FALSE,
                            chains = 4, cores = 4, threads = 4,
                            warmup = 750, iter = 1500, seed = 1,
                            open_progress = TRUE,
                            anchors = c(1, 2))})
# saveRDS(fitted_model, "data/fitted_models/MOC115_fitted_model.rds")
```

<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r echo = FALSE, eval = FALSE}
# This is a _local_ file for the vignette only
fitted_model <- readRDS("~/GitHub/mediascores/data/fitted_models/MOC115_fitted_model.rds")
```



# 5. Model Diagnostics {#section5}

The object returned from the `mediascores()` function is a standard [Stan](https://mc-stan.org){target="_blank"} object. For in depth analysis of the posterior, one can therefore treat it as such and use open-source visualization tools to examine it, such as [`shinystan`](https://mc-stan.org/users/interfaces/shinystan){target="_blank"}:

```{r eval = FALSE}
library(shinystan)
shiny_stan(fitted_model)
```

Here, we focus on a simple helper function built into the [`mediascores`](https://cran.r-project.org/package=mediascores){target="_blank"} library to access the convergence statistic, $\hat{R}$, which is the commonly used means of assessing whether one's parameters of interest have converged. We can use the `rhat()` function to display the $\hat{R}$ statistic for the two typical quantities of interest, $\vartheta_i$ (the sharing ideology of users), and $\zeta_m$ (the ideology of news media domains):

```{r eval = FALSE}
rhats <- rhat(posterior = fitted_model, pars = c("zeta", "theta"))
print(rhats)
``` 

<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r echo = FALSE, eval = FALSE}
rhats <- rhat(posterior = fitted_model, pars = c("theta", "zeta"))
{print(subset(rhats, rownames(rhats) %in% c("zeta[1]", "zeta[2]", "zeta[3]", "zeta[4]", "zeta[5]")))
cat("...\n")
print(subset(rhats, rownames(rhats) %in% paste0("theta[", (nrow(User_Domain_Counts)-4):nrow(User_Domain_Counts), "]")))}
```

The function outputs the $\hat{R}$ statistics for all the specified parameters, and includes a warning for any of those parameters that are greater than 1.1, the rule-of-thumb cutoff suggested by @Gelman2014. To see if any of our $\hat{R}$ statistics are greater than 1.1 we can simply look at the subset:

<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r, eval = FALSE}
subset(rhats, rhat > 1.1)
```

Here, we can see that none of those parameters have $\hat{R}$ statistics greater than 1.1. If we did, however, it is generally suggested that the model be re-run to collect more post-warmup iterations:

```{r eval = FALSE}
# Setting iter = 2000 (not run)
fitted_model <- mediascores(Y = User_Domain_Counts, group = MOC115$group,
                            variational = FALSE, user_variance = FALSE,
                            warmup = 750, iter = 2000, chains = 4,
                            open_progress = TRUE, seed = 1,
                            # output_samples = 5000, tol_rel_obj = 0.001,
                            anchors = c(1, 2))
```

For more detail on $\hat{R}$, see the [Stan Modeling Language User Guide and Reference](https://mc-stan.org/users/documentation/){target="_blank"}.


## 5.1 Divergent transitions {#subsection51}

If you receive a message stating that there were "divergent transitions" during estimation, this suggests potential validity concerns about the resulting parameter estimates. Details of this issue lie outside the purview of this vignette, but it can typically be resolved by increasing the value of the `adapt_delta`, which can be included in the `mediascores()` function as follows: `mediascores(..., control = list(adapt_delta = 0.99))`. The default value is 0.8 (with a maximum less than 1). Values of 0.99 or 0.999 typically resolve the problem. However, note that increasing the value of `adapt_delta` can substantially increase the time for model fitting, which is computationally demanding as is, especially with relatively large datasets. In practice for the model presented in this library, this problem appears to be rare (we have yet to encounter it).

## 5.2 Maximum treedepth exceeded {#subsection52}

If you receive a message stating that the "maximum treedepth" was frequently exceeded during estimation, the concern is not validity, but efficiency. If desired, you can increase efficiency by increasing the value of the `max_treedepth` argument, which defaults to 10. As with `adapt_delta`, this argument can be included thusly: `mediascores(..., control = list(max_treedepth = 15))`.


# 6. Analysis {#section6}

Finally, we examine the estimates of our quantities of interest: (1) the sharing ideology of Members of Congress, and (2) the ideology of the news media content that they share.

To pull out these estimates, we can use the `point_est()` function, which calculates the median of the posterior of each parameter of interest, and provides credible intervals:

```{r eval = FALSE}
theta_hat <- point_est(fitted_model, pars = c("theta"), prob = 0.9)
print(theta_hat)
```

<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r echo = FALSE, eval = FALSE}
theta_hat <- point_est(fitted_model, pars = c("theta"), prob = 0.9)
{print(subset(theta_hat, rownames(theta_hat) %in% c("theta[1]", "theta[2]", "theta[3]", "theta[4]", "theta[5]")))
cat("...\n")
print(subset(theta_hat, rownames(theta_hat) %in% paste0("theta[", (nrow(User_Domain_Counts)-4):nrow(User_Domain_Counts), "]")))}
```

Because these data are ordered as they are in our original data, we can merge them into our politicians dataset and see how news sharing ideology aligns with voting ideology (i.e. NOMINATE scores):

<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r, eval = FALSE}
MOC115$theta_hat <- theta_hat[, "median"]

# All politicians
round(cor(MOC115$theta_hat, MOC115$nominate, use = "complete.obs"), 2)

# Democrats
round(cor(MOC115$theta_hat[MOC115$party == "Democrat"], MOC115$nominate[MOC115$party == "Democrat"], use = "complete.obs"), 2)

# Republicans
round(cor(MOC115$theta_hat[MOC115$party == "Republican"], MOC115$nominate[MOC115$party == "Republican"], use = "complete.obs"), 2)
```

We can further graph these estimates as follows:

```{r, eval = FALSE}
ggplot(subset(MOC115, role %in% c("Senate", "House")),
       aes(x = theta_hat, y = nominate, color = party,
           fill = party, shape = party)) +
  facet_wrap(~ role) +
  labs(x = expression(bold(paste("Media score (", vartheta, ")"))),
       y = "DW-NOMINATE") +
  coord_cartesian(xlim = c(-1.1, 1.6), ylim = c(-1.1, 1.1), expand = FALSE) +
  geom_point(size = 2.25, shape = 21, stroke = 0.4) +
  scale_colour_manual(values = c("Republican" = "red", "Democrat" = "white")) +
  scale_fill_manual(values = c("Republican" = NA, "Democrat" = "blue")) +
  theme(legend.position = "none")
```

<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r, echo = FALSE, out.width = "100%", fig.height = 3.75, warning = FALSE, eval = FALSE}
ggplot(subset(MOC115, role %in% c("Senate", "House")),
       aes(x = theta_hat, y = nominate, color = party,
           fill = party, shape = party)) +
  my.theme(base_size = 11, borderless = 2,
           remove.ticks.x = TRUE, remove.ticks.y = TRUE,
           grid.x_colour = "grey50", grid.y_colour = "grey50",
           grid.x_linetype = 3, grid.y_linetype = 3) +
  facet_wrap(~ role) +
  labs(x = expression(bold(paste("Media score (", vartheta, ")"))),
       y = "DW-NOMINATE") +
  coord_cartesian(xlim = c(-1.1, 1.6), ylim = c(-1.1, 1.1), expand = FALSE) +
  geom_point(size = 2.5, shape = 21, stroke = 0.4) +
  scale_colour_manual(values = c("Republican" = "red", "Democrat" = "white")) +
  scale_fill_manual(values = c("Republican" = NA, "Democrat" = "blue")) +
  theme(legend.position = "none")
```

We can also examine estimates of the ideology of the news media domains as follows:

<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r, eval = FALSE}
zeta_hat <- point_est(fitted_model, "zeta", prob = 0.9)

# Collect the estimates of news media ideology in a data.frame
M <- data.frame(media_org = colnames(MOC115)[7:(ncol(MOC115)-1)],
                zeta_hat = zeta_hat[, "median"],
                zeta_hat_5 = zeta_hat[, "5%"],
                zeta_hat_95 = zeta_hat[, "95%"])

# Re-order media_org for plotting from low values of zeta to high
M$media_org <- reorder(M$media_org, -M$zeta_hat)
```

Graph news media ideology estimates with 90% credible intervals:

```{r, eval = FALSE}
M$label <- "right"
M$label[M$zeta_hat > median(M$zeta_hat)] <- "left"

ggplot(M, aes(x = zeta_hat, y = media_org)) +
  coord_cartesian(xlim = c(-3.4, 3.4)) +
  scale_y_discrete(breaks = c()) +
  scale_x_continuous(breaks = -3:3) +
  labs(y = "", x = expression(bold(paste("Media score (", zeta, ")")))) +
  geom_segment(aes(x = zeta_hat_5, xend = zeta_hat_95, y = media_org, yend = media_org),
                   size = 2, color = "grey85") +
  geom_point(size = 1.25, shape = 21, stroke = 0.4, color = "black",
             fill = "black", shape = 21) +
  geom_label(data = subset(M, label == "left"),
             aes(x = zeta_hat_5, label = media_org), color = "black",
             label.r = unit(0, "lines"), label.size = NA,
             label.padding = unit(0.07, "lines"), 
             hjust = 1, size = 2.5, nudge_x = -0.13) +
  geom_label(data = subset(M, label == "right"),
             aes(x = zeta_hat_95, label = media_org), color = "black",
             label.r = unit(0, "lines"), label.size = NA,
             label.padding = unit(0.07, "lines"), 
             hjust = 0, size = 2.5, nudge_x = 0.13)
```


<!-- SET TO EVAL = TRUE TO BUILD VIGNETTE -->
```{r, echo = FALSE, fig.height = 15, out.width = "100%", warning = FALSE, eval = FALSE}
M$label <- "right"
M$label[M$zeta_hat > median(M$zeta_hat)] <- "left"

ggplot(M, aes(x = zeta_hat, y = media_org)) +
  my.theme(base_size = 11, borderless = 2,
           remove.ticks.x = TRUE,
           grid.x_linetype = 3) +
  coord_cartesian(xlim = c(-3.4, 3.4)) +
  scale_y_discrete(breaks = c()) +
  scale_x_continuous(breaks = -3:3) +
  labs(y = "", x = expression(bold(paste("Media score (", zeta, ")")))) +
  geom_segment(aes(x = zeta_hat_5, xend = zeta_hat_95, y = media_org, yend = media_org),
                   size = 2, color = "grey85") +
  geom_point(size = 1.25, shape = 21, stroke = 0.4, color = "black",
             fill = "black") +
  geom_label(data = subset(M, label == "left"),
             aes(x = zeta_hat_5, label = media_org), color = "black",
             label.r = unit(0, "lines"), label.size = NA,
             label.padding = unit(0.07, "lines"), 
             hjust = 1, size = 2.5, nudge_x = -0.13) +
  geom_label(data = subset(M, label == "right"),
             aes(x = zeta_hat_95, label = media_org), color = "black",
             label.r = unit(0, "lines"), label.size = NA,
             label.padding = unit(0.07, "lines"), 
             hjust = 0, size = 2.5, nudge_x = 0.13)
```

# 7. A Note on CPU Parallelization {#section7}

The [`mediascores`](https://cran.r-project.org/package=mediascores){target="_blank"} library is coded to use map-reduce to permit within-chain CPU parallelization for estimation. In effect, this means that you can use many CPU cores for model fitting, which can _substantially_ cut down on estimation time. This is especially useful even for medium-sized data sets, because estimation is computationally demanding. The drawback is that to turn on parallelization, the C++ code compiled during the library's installation must be done with flags to enable CPU parallelization. The process to do this varies from system to system (e.g. Windows, Unix, Mac). For support, please consult the following resources: [Stan Threading Support](https://github.com/stan-dev/math/wiki/Threading-Support){target="_blank"}, [Stan MPI Parallelism](https://github.com/stan-dev/math/wiki/Threading-Support){target="_blank"} (for cluster-based systems), and the [Stan user forums](https://discourse.mc-stan.org){target="_blank"}.

Note that enabling parallelization, however, is generally straightforward. For example, enabling parallelization on the systems to which the author has access required only that the following be added to the `~/.R/Makevars` file (create this file if it does not exist):

On Mac OS (10.14.3):

```
CXX14FLAGS=-O3 -march=native -mtune=native
CXX14FLAGS += -arch x86_64 -ftemplate-depth-256
CXX14FLAGS += -DSTAN_THREADS
```

On CentOS 7:

```
CXX14 = icc
CXX14FLAGS = -DSTAN_THREADS
CXX14FLAGS += -O3 -march=native -mtune=native
CXX14FLAGS += -fPIC
```


# References
<hr style = "margin-bottom: 22px; margin-top: 12px; border:1px; border-top: dotted 1px; color:#006400">

